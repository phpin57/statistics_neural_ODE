{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e04cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "#from torchdiffeq import odeint_adjoint as odeint\n",
    "from scipy.linalg import expm\n",
    "from torchdiffeq import odeint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de0eee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dydt(y, t, A):\n",
    "    return torch.mm(y,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "537d6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_A(y,A):\n",
    "    return odeint(lambda t,x : dydt(x,t,A), y, torch.tensor([0., 1.]))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e25b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(torch.nn.Module):\n",
    "    def __init__(self, A):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.A = torch.nn.Parameter(torch.tensor(A))\n",
    "        \n",
    "    def forward(self, t, y):\n",
    "        return dydt(y, t, self.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7196cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(torch.nn.Module):\n",
    "    def __init__(self, A_init):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.func = ODEFunc(A_init)\n",
    "        self.hidden_layer = torch.nn.Linear(2, 10)\n",
    "        self.output_layer = torch.nn.Linear(10, 4)\n",
    "        \n",
    "    def forward(self, y):\n",
    "        y = self.hidden_layer(y)\n",
    "        y = torch.relu(y)\n",
    "        y = self.output_layer(y)\n",
    "        return y\n",
    "    \n",
    "    def get_A(self):\n",
    "        return self.func.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c3bc567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_true = torch.tensor([[1., 2.], [3., 4.]])\n",
    "training_losses={}\n",
    "frob_losses={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "96f7ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_data,y_data,epochs=500, lr=0.01):\n",
    "    training_loss=[]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = odeint(model.func, x_data, torch.tensor([0., 1.]), method='dopri5')\n",
    "        loss = criterion(y_pred, y_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Training Loss: {loss:.4f}\")\n",
    "            print(neural_ode.get_A())\n",
    "            training_loss.append(int(loss.detach().numpy().item()))\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3904b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=100\n",
    "\n",
    "x_data = torch.randn(n_samples, 2)\n",
    "y_data = phi_A(x_data,A_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0b37c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12628\\3190449401.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.A = torch.nn.Parameter(torch.tensor(A))\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([100, 2])) that is different to the input size (torch.Size([2, 100, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 21579.8750\n",
      "Parameter containing:\n",
      "tensor([[1.0100, 0.0100],\n",
      "        [0.0100, 1.0100]], requires_grad=True)\n",
      "Epoch 20, Training Loss: 21457.0117\n",
      "Parameter containing:\n",
      "tensor([[1.2167, 0.2159],\n",
      "        [0.2147, 1.2140]], requires_grad=True)\n",
      "Epoch 40, Training Loss: 21249.0430\n",
      "Parameter containing:\n",
      "tensor([[1.4526, 0.4478],\n",
      "        [0.4417, 1.4373]], requires_grad=True)\n",
      "Epoch 60, Training Loss: 20861.2578\n",
      "Parameter containing:\n",
      "tensor([[1.7301, 0.7187],\n",
      "        [0.7044, 1.6927]], requires_grad=True)\n",
      "Epoch 80, Training Loss: 20088.9355\n",
      "Parameter containing:\n",
      "tensor([[2.0524, 1.0333],\n",
      "        [1.0100, 1.9891]], requires_grad=True)\n",
      "Epoch 100, Training Loss: 18516.3359\n",
      "Parameter containing:\n",
      "tensor([[2.4160, 1.3905],\n",
      "        [1.3598, 2.3304]], requires_grad=True)\n",
      "Epoch 120, Training Loss: 15634.7412\n",
      "Parameter containing:\n",
      "tensor([[2.8055, 1.7776],\n",
      "        [1.7437, 2.7093]], requires_grad=True)\n",
      "Epoch 140, Training Loss: 12865.5820\n",
      "Parameter containing:\n",
      "tensor([[3.1228, 2.1219],\n",
      "        [2.1071, 3.0803]], requires_grad=True)\n",
      "Epoch 160, Training Loss: 12303.7949\n",
      "Parameter containing:\n",
      "tensor([[2.9888, 2.1338],\n",
      "        [2.2579, 3.2967]], requires_grad=True)\n",
      "Epoch 180, Training Loss: 11829.8477\n",
      "Parameter containing:\n",
      "tensor([[2.7587, 2.0343],\n",
      "        [2.3207, 3.4540]], requires_grad=True)\n",
      "Epoch 200, Training Loss: 11503.0303\n",
      "Parameter containing:\n",
      "tensor([[2.6047, 1.9827],\n",
      "        [2.3928, 3.6194]], requires_grad=True)\n",
      "Epoch 220, Training Loss: 11279.3135\n",
      "Parameter containing:\n",
      "tensor([[2.4597, 1.9108],\n",
      "        [2.4291, 3.7492]], requires_grad=True)\n",
      "Epoch 240, Training Loss: 11128.9590\n",
      "Parameter containing:\n",
      "tensor([[2.3413, 1.8482],\n",
      "        [2.4514, 3.8607]], requires_grad=True)\n",
      "Epoch 260, Training Loss: 11030.6992\n",
      "Parameter containing:\n",
      "tensor([[2.2433, 1.7924],\n",
      "        [2.4613, 3.9522]], requires_grad=True)\n",
      "Epoch 280, Training Loss: 10968.0820\n",
      "Parameter containing:\n",
      "tensor([[2.1634, 1.7450],\n",
      "        [2.4629, 4.0264]], requires_grad=True)\n",
      "Epoch 300, Training Loss: 10929.1953\n",
      "Parameter containing:\n",
      "tensor([[2.0990, 1.7062],\n",
      "        [2.4597, 4.0859]], requires_grad=True)\n",
      "Epoch 320, Training Loss: 10905.6279\n",
      "Parameter containing:\n",
      "tensor([[2.0476, 1.6755],\n",
      "        [2.4537, 4.1327]], requires_grad=True)\n",
      "Epoch 340, Training Loss: 10891.6826\n",
      "Parameter containing:\n",
      "tensor([[2.0070, 1.6517],\n",
      "        [2.4465, 4.1692]], requires_grad=True)\n",
      "Epoch 360, Training Loss: 10883.6104\n",
      "Parameter containing:\n",
      "tensor([[1.9753, 1.6339],\n",
      "        [2.4392, 4.1972]], requires_grad=True)\n",
      "Epoch 380, Training Loss: 10879.0400\n",
      "Parameter containing:\n",
      "tensor([[1.9508, 1.6209],\n",
      "        [2.4323, 4.2185]], requires_grad=True)\n",
      "Epoch 400, Training Loss: 10876.5059\n",
      "Parameter containing:\n",
      "tensor([[1.9320, 1.6115],\n",
      "        [2.4263, 4.2345]], requires_grad=True)\n",
      "Epoch 420, Training Loss: 10875.1289\n",
      "Parameter containing:\n",
      "tensor([[1.9178, 1.6051],\n",
      "        [2.4212, 4.2463]], requires_grad=True)\n",
      "Epoch 440, Training Loss: 10874.3945\n",
      "Parameter containing:\n",
      "tensor([[1.9071, 1.6007],\n",
      "        [2.4170, 4.2550]], requires_grad=True)\n",
      "Epoch 460, Training Loss: 10874.0117\n",
      "Parameter containing:\n",
      "tensor([[1.8990, 1.5978],\n",
      "        [2.4137, 4.2612]], requires_grad=True)\n",
      "Epoch 480, Training Loss: 10873.8154\n",
      "Parameter containing:\n",
      "tensor([[1.8930, 1.5959],\n",
      "        [2.4111, 4.2657]], requires_grad=True)\n",
      "Estimated A:\n",
      "Parameter containing:\n",
      "tensor([[1.8888, 1.5948],\n",
      "        [2.4093, 4.2687]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "neural_ode = NeuralODE(torch.eye(2))\n",
    "\n",
    "training_loss=train_model(neural_ode, x_data,y_data)\n",
    "A_estimated = neural_ode.get_A()\n",
    "\n",
    "#ajout des r√©sultats pour chaque n_samples\n",
    "training_losses[n_samples]=training_loss\n",
    "frob_losses[n_samples]=np.linalg.norm((A_true-A_estimated).detach().numpy())\n",
    "\n",
    "print(f\"Estimated A:\\n{A_estimated}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
