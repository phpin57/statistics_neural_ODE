{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a14ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "#from torchdiffeq import odeint_adjoint as odeint\n",
    "from scipy.linalg import expm\n",
    "from torchdiffeq import odeint\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ba1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dydt(y, t, A):\n",
    "    return torch.mm(y,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc13465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_A(y,A):\n",
    "    return odeint(lambda t,x : dydt(x,t,A), y, torch.tensor([0., 1.]))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c29990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(torch.nn.Module):\n",
    "    def __init__(self, A):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.A = torch.nn.Parameter(torch.tensor(A))\n",
    "        \n",
    "    def forward(self, t, y):\n",
    "        return dydt(y, t, self.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "36e67112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(torch.nn.Module):\n",
    "    def __init__(self, A_init):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.func = ODEFunc(A_init)\n",
    "        self.dim=len(A_init)\n",
    "        self.hidden_layer = torch.nn.Linear(self.dim, 100)\n",
    "        self.output_layer = torch.nn.Linear(100, self.dim*self.dim)\n",
    "        \n",
    "    def forward(self, y):\n",
    "        y = self.hidden_layer(y)\n",
    "        y = torch.relu(y)\n",
    "        y = self.output_layer(y)\n",
    "        return y\n",
    "    \n",
    "    def get_A(self):\n",
    "        return self.func.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e8c9317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses={}\n",
    "frob_losses={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3e56ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "mean = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7020f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_data,y_data,epochs=300, lr=0.05):\n",
    "    training_loss=[]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = odeint(model.func, x_data, torch.tensor([0., 1.]), method='dopri5')\n",
    "        loss = criterion(y_pred, y_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Training Loss: {loss:.4f}\")\n",
    "            print(torch.diag(neural_ode.get_A()))\n",
    "            training_loss.append(round(loss.detach().numpy().item(),3))\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f803bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stddevs=[i/10 for i in range(1,11)]\n",
    "x_data = torch.randn(n_samples, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "10650a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "déviation standard :  0.1\n",
      "matrice à viser :  tensor([0.7909, 0.9622, 0.9909, 1.1709, 0.8245, 0.9745, 1.0377, 1.0664, 0.8136,\n",
      "        1.0547])\n",
      "Epoch 0, Training Loss: 1.4688\n",
      "tensor([0.9500, 0.9500, 0.9500, 1.0500, 0.9500, 0.9500, 1.0500, 1.0500, 0.9500,\n",
      "        1.0500], grad_fn=<DiagBackward0>)\n",
      "Epoch 20, Training Loss: 1.4231\n",
      "tensor([0.8198, 0.9608, 0.9685, 1.1573, 0.8724, 0.9778, 1.0434, 1.0565, 0.8586,\n",
      "        1.0475], grad_fn=<DiagBackward0>)\n",
      "Epoch 40, Training Loss: 1.4186\n",
      "tensor([0.7736, 0.9617, 0.9933, 1.1720, 0.8337, 0.9684, 1.0488, 1.0689, 0.8147,\n",
      "        1.0542], grad_fn=<DiagBackward0>)\n",
      "Epoch 60, Training Loss: 1.4181\n",
      "tensor([0.7823, 0.9628, 0.9916, 1.1756, 0.8269, 0.9739, 1.0377, 1.0665, 0.8113,\n",
      "        1.0565], grad_fn=<DiagBackward0>)\n",
      "Epoch 80, Training Loss: 1.4180\n",
      "tensor([0.7880, 0.9620, 0.9898, 1.1711, 0.8262, 0.9741, 1.0371, 1.0666, 0.8131,\n",
      "        1.0547], grad_fn=<DiagBackward0>)\n",
      "Epoch 100, Training Loss: 1.4180\n",
      "tensor([0.7899, 0.9620, 0.9906, 1.1704, 0.8254, 0.9747, 1.0373, 1.0660, 0.8140,\n",
      "        1.0545], grad_fn=<DiagBackward0>)\n",
      "Epoch 120, Training Loss: 1.4180\n",
      "tensor([0.7905, 0.9621, 0.9908, 1.1711, 0.8246, 0.9745, 1.0376, 1.0664, 0.8140,\n",
      "        1.0547], grad_fn=<DiagBackward0>)\n",
      "Epoch 140, Training Loss: 1.4180\n",
      "tensor([0.7908, 0.9622, 0.9909, 1.1708, 0.8244, 0.9745, 1.0377, 1.0663, 0.8137,\n",
      "        1.0547], grad_fn=<DiagBackward0>)\n",
      "Epoch 160, Training Loss: 1.4180\n",
      "tensor([0.7909, 0.9622, 0.9909, 1.1709, 0.8245, 0.9745, 1.0377, 1.0664, 0.8136,\n",
      "        1.0547], grad_fn=<DiagBackward0>)\n",
      "Epoch 180, Training Loss: 1.4180\n",
      "tensor([0.7909, 0.9622, 0.9909, 1.1709, 0.8245, 0.9745, 1.0377, 1.0664, 0.8136,\n",
      "        1.0547], grad_fn=<DiagBackward0>)\n",
      "Epoch 200, Training Loss: 1.4180\n",
      "tensor([0.7909, 0.9622, 0.9909, 1.1709, 0.8245, 0.9745, 1.0377, 1.0664, 0.8136,\n",
      "        1.0547], grad_fn=<DiagBackward0>)\n",
      "Epoch 220, Training Loss: 1.4180\n",
      "tensor([0.7909, 0.9622, 0.9909, 1.1709, 0.8245, 0.9745, 1.0377, 1.0664, 0.8136,\n",
      "        1.0547], grad_fn=<DiagBackward0>)\n",
      "Epoch 240, Training Loss: 1.4180\n",
      "tensor([0.7909, 0.9622, 0.9909, 1.1709, 0.8245, 0.9745, 1.0377, 1.0664, 0.8136,\n",
      "        1.0547], grad_fn=<DiagBackward0>)\n",
      "Epoch 260, Training Loss: 1.4180\n",
      "tensor([0.7909, 0.9622, 0.9909, 1.1709, 0.8245, 0.9745, 1.0377, 1.0664, 0.8136,\n",
      "        1.0547], grad_fn=<DiagBackward0>)\n",
      "Epoch 280, Training Loss: 1.4180\n",
      "tensor([0.7909, 0.9622, 0.9909, 1.1709, 0.8245, 0.9745, 1.0377, 1.0664, 0.8136,\n",
      "        1.0547], grad_fn=<DiagBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:15<02:16, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.1': 5.050203e-07, '0.2': 1.875338e-06, '0.3': 1.1974118e-05, '0.4': 0.00038350897, '0.5': 5.9100126e-05, '0.6': 8.115813}\n",
      "déviation standard :  0.2\n",
      "matrice à viser :  tensor([0.9543, 0.8534, 0.8984, 0.6965, 0.8378, 1.0229, 0.9841, 0.8166, 0.7295,\n",
      "        0.6387])\n",
      "Epoch 0, Training Loss: 1.0488\n",
      "tensor([0.9500, 0.9500, 0.9500, 0.9500, 0.9500, 1.0500, 0.9500, 0.9500, 0.9500,\n",
      "        0.9500], grad_fn=<DiagBackward0>)\n",
      "Epoch 20, Training Loss: 0.9475\n",
      "tensor([0.9410, 0.8934, 0.8774, 0.6181, 0.8873, 1.0343, 0.9639, 0.8616, 0.6858,\n",
      "        0.5127], grad_fn=<DiagBackward0>)\n",
      "Epoch 40, Training Loss: 0.9384\n",
      "tensor([0.9510, 0.8703, 0.8844, 0.7048, 0.8529, 1.0318, 0.9838, 0.8175, 0.7141,\n",
      "        0.6803], grad_fn=<DiagBackward0>)\n",
      "Epoch 60, Training Loss: 0.9375\n",
      "tensor([0.9532, 0.8590, 0.8988, 0.7050, 0.8434, 1.0235, 0.9852, 0.8140, 0.7399,\n",
      "        0.6257], grad_fn=<DiagBackward0>)\n",
      "Epoch 80, Training Loss: 0.9374\n",
      "tensor([0.9544, 0.8537, 0.8994, 0.6927, 0.8403, 1.0230, 0.9834, 0.8160, 0.7316,\n",
      "        0.6395], grad_fn=<DiagBackward0>)\n",
      "Epoch 100, Training Loss: 0.9374\n",
      "tensor([0.9543, 0.8526, 0.8980, 0.6954, 0.8383, 1.0229, 0.9838, 0.8169, 0.7289,\n",
      "        0.6403], grad_fn=<DiagBackward0>)\n",
      "Epoch 120, Training Loss: 0.9374\n",
      "tensor([0.9543, 0.8534, 0.8985, 0.6967, 0.8375, 1.0230, 0.9840, 0.8170, 0.7290,\n",
      "        0.6381], grad_fn=<DiagBackward0>)\n",
      "Epoch 140, Training Loss: 0.9374\n",
      "tensor([0.9543, 0.8535, 0.8984, 0.6967, 0.8377, 1.0228, 0.9841, 0.8167, 0.7293,\n",
      "        0.6385], grad_fn=<DiagBackward0>)\n",
      "Epoch 160, Training Loss: 0.9374\n",
      "tensor([0.9543, 0.8534, 0.8984, 0.6966, 0.8378, 1.0229, 0.9842, 0.8166, 0.7294,\n",
      "        0.6387], grad_fn=<DiagBackward0>)\n",
      "Epoch 180, Training Loss: 0.9374\n",
      "tensor([0.9543, 0.8534, 0.8984, 0.6965, 0.8377, 1.0229, 0.9841, 0.8166, 0.7294,\n",
      "        0.6387], grad_fn=<DiagBackward0>)\n",
      "Epoch 200, Training Loss: 0.9374\n",
      "tensor([0.9543, 0.8534, 0.8984, 0.6965, 0.8378, 1.0229, 0.9841, 0.8166, 0.7294,\n",
      "        0.6387], grad_fn=<DiagBackward0>)\n",
      "Epoch 220, Training Loss: 0.9374\n",
      "tensor([0.9543, 0.8534, 0.8984, 0.6965, 0.8378, 1.0229, 0.9841, 0.8166, 0.7295,\n",
      "        0.6387], grad_fn=<DiagBackward0>)\n",
      "Epoch 240, Training Loss: 0.9374\n",
      "tensor([0.9543, 0.8534, 0.8984, 0.6965, 0.8378, 1.0229, 0.9841, 0.8166, 0.7295,\n",
      "        0.6387], grad_fn=<DiagBackward0>)\n",
      "Epoch 260, Training Loss: 0.9374\n",
      "tensor([0.9543, 0.8534, 0.8984, 0.6965, 0.8378, 1.0229, 0.9841, 0.8166, 0.7295,\n",
      "        0.6387], grad_fn=<DiagBackward0>)\n",
      "Epoch 280, Training Loss: 0.9374\n",
      "tensor([0.9543, 0.8534, 0.8984, 0.6965, 0.8378, 1.0229, 0.9841, 0.8166, 0.7295,\n",
      "        0.6387], grad_fn=<DiagBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [01:16<05:40, 42.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.1': 5.050203e-07, '0.2': 1.3678467e-06, '0.3': 1.1974118e-05, '0.4': 0.00038350897, '0.5': 5.9100126e-05, '0.6': 8.115813}\n",
      "déviation standard :  0.3\n",
      "matrice à viser :  tensor([1.5326, 1.1159, 0.9336, 0.5357, 0.8207, 0.9462, 0.8657, 1.4690, 0.7668,\n",
      "        0.8616])\n",
      "Epoch 0, Training Loss: 2.3751\n",
      "tensor([1.0500, 1.0500, 0.9500, 0.9500, 0.9500, 0.9500, 0.9500, 1.0500, 0.9500,\n",
      "        0.9500], grad_fn=<DiagBackward0>)\n",
      "Epoch 20, Training Loss: 2.0031\n",
      "tensor([1.6229, 1.1423, 0.9145, 0.3867, 0.8678, 0.9406, 0.8918, 1.5244, 0.7700,\n",
      "        0.8927], grad_fn=<DiagBackward0>)\n",
      "Epoch 40, Training Loss: 1.9792\n",
      "tensor([1.5244, 1.1124, 0.9356, 0.5488, 0.8266, 0.9410, 0.8762, 1.4826, 0.7403,\n",
      "        0.8739], grad_fn=<DiagBackward0>)\n",
      "Epoch 60, Training Loss: 1.9786\n",
      "tensor([1.5185, 1.1163, 0.9360, 0.5352, 0.8212, 0.9459, 0.8659, 1.4568, 0.7632,\n",
      "        0.8630], grad_fn=<DiagBackward0>)\n",
      "Epoch 80, Training Loss: 1.9782\n",
      "tensor([1.5338, 1.1152, 0.9336, 0.5354, 0.8215, 0.9468, 0.8637, 1.4653, 0.7680,\n",
      "        0.8598], grad_fn=<DiagBackward0>)\n",
      "Epoch 100, Training Loss: 1.9782\n",
      "tensor([1.5345, 1.1164, 0.9333, 0.5352, 0.8215, 0.9459, 0.8657, 1.4689, 0.7676,\n",
      "        0.8613], grad_fn=<DiagBackward0>)\n",
      "Epoch 120, Training Loss: 1.9782\n",
      "tensor([1.5332, 1.1160, 0.9335, 0.5362, 0.8210, 0.9461, 0.8659, 1.4692, 0.7671,\n",
      "        0.8619], grad_fn=<DiagBackward0>)\n",
      "Epoch 140, Training Loss: 1.9782\n",
      "tensor([1.5328, 1.1159, 0.9336, 0.5353, 0.8206, 0.9462, 0.8656, 1.4691, 0.7669,\n",
      "        0.8615], grad_fn=<DiagBackward0>)\n",
      "Epoch 160, Training Loss: 1.9782\n",
      "tensor([1.5326, 1.1159, 0.9336, 0.5358, 0.8206, 0.9462, 0.8658, 1.4690, 0.7668,\n",
      "        0.8617], grad_fn=<DiagBackward0>)\n",
      "Epoch 180, Training Loss: 1.9782\n",
      "tensor([1.5326, 1.1159, 0.9336, 0.5356, 0.8207, 0.9462, 0.8657, 1.4690, 0.7668,\n",
      "        0.8616], grad_fn=<DiagBackward0>)\n",
      "Epoch 200, Training Loss: 1.9782\n",
      "tensor([1.5326, 1.1159, 0.9336, 0.5357, 0.8207, 0.9462, 0.8657, 1.4690, 0.7668,\n",
      "        0.8616], grad_fn=<DiagBackward0>)\n",
      "Epoch 220, Training Loss: 1.9782\n",
      "tensor([1.5326, 1.1159, 0.9336, 0.5357, 0.8207, 0.9462, 0.8657, 1.4690, 0.7668,\n",
      "        0.8616], grad_fn=<DiagBackward0>)\n",
      "Epoch 240, Training Loss: 1.9782\n",
      "tensor([1.5326, 1.1159, 0.9336, 0.5357, 0.8207, 0.9462, 0.8657, 1.4690, 0.7668,\n",
      "        0.8616], grad_fn=<DiagBackward0>)\n",
      "Epoch 260, Training Loss: 1.9782\n",
      "tensor([1.5326, 1.1159, 0.9336, 0.5357, 0.8207, 0.9462, 0.8657, 1.4690, 0.7668,\n",
      "        0.8616], grad_fn=<DiagBackward0>)\n",
      "Epoch 280, Training Loss: 1.9782\n",
      "tensor([1.5326, 1.1159, 0.9336, 0.5357, 0.8207, 0.9462, 0.8657, 1.4690, 0.7668,\n",
      "        0.8616], grad_fn=<DiagBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [02:26<06:22, 54.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.1': 5.050203e-07, '0.2': 1.3678467e-06, '0.3': 2.8308523e-05, '0.4': 0.00038350897, '0.5': 5.9100126e-05, '0.6': 8.115813}\n",
      "déviation standard :  0.4\n",
      "matrice à viser :  tensor([0.8885, 1.3138, 1.1347, 0.4187, 1.2640, 1.3893, 1.2619, 1.7043, 1.0957,\n",
      "        0.5964])\n",
      "Epoch 0, Training Loss: 3.7626\n",
      "tensor([0.9500, 1.0500, 1.0500, 0.9500, 1.0500, 1.0500, 1.0500, 1.0500, 1.0500,\n",
      "        0.9500], grad_fn=<DiagBackward0>)\n",
      "Epoch 20, Training Loss: 3.0889\n",
      "tensor([0.8765, 1.2730, 1.1578, 0.3015, 1.2105, 1.3908, 1.2128, 1.8373, 1.0988,\n",
      "        0.4562], grad_fn=<DiagBackward0>)\n",
      "Epoch 40, Training Loss: 3.0571\n",
      "tensor([0.8762, 1.3252, 1.1408, 0.3548, 1.2551, 1.4180, 1.2488, 1.6498, 1.0984,\n",
      "        0.6387], grad_fn=<DiagBackward0>)\n",
      "Epoch 60, Training Loss: 3.0506\n",
      "tensor([0.8847, 1.3215, 1.1313, 0.4463, 1.2619, 1.3925, 1.2567, 1.7115, 1.0933,\n",
      "        0.5791], grad_fn=<DiagBackward0>)\n",
      "Epoch 80, Training Loss: 3.0502\n",
      "tensor([0.8903, 1.3167, 1.1359, 0.4123, 1.2624, 1.3880, 1.2596, 1.7099, 1.0954,\n",
      "        0.6026], grad_fn=<DiagBackward0>)\n",
      "Epoch 100, Training Loss: 3.0501\n",
      "tensor([0.8881, 1.3146, 1.1344, 0.4192, 1.2631, 1.3885, 1.2617, 1.7027, 1.0956,\n",
      "        0.5950], grad_fn=<DiagBackward0>)\n",
      "Epoch 120, Training Loss: 3.0501\n",
      "tensor([0.8887, 1.3139, 1.1346, 0.4190, 1.2639, 1.3890, 1.2622, 1.7035, 1.0956,\n",
      "        0.5961], grad_fn=<DiagBackward0>)\n",
      "Epoch 140, Training Loss: 3.0501\n",
      "tensor([0.8885, 1.3137, 1.1347, 0.4185, 1.2642, 1.3892, 1.2619, 1.7043, 1.0957,\n",
      "        0.5966], grad_fn=<DiagBackward0>)\n",
      "Epoch 160, Training Loss: 3.0501\n",
      "tensor([0.8885, 1.3138, 1.1347, 0.4188, 1.2640, 1.3893, 1.2619, 1.7044, 1.0957,\n",
      "        0.5963], grad_fn=<DiagBackward0>)\n",
      "Epoch 180, Training Loss: 3.0501\n",
      "tensor([0.8885, 1.3138, 1.1347, 0.4187, 1.2640, 1.3893, 1.2619, 1.7044, 1.0957,\n",
      "        0.5963], grad_fn=<DiagBackward0>)\n",
      "Epoch 200, Training Loss: 3.0501\n",
      "tensor([0.8885, 1.3138, 1.1347, 0.4187, 1.2641, 1.3893, 1.2619, 1.7043, 1.0957,\n",
      "        0.5964], grad_fn=<DiagBackward0>)\n",
      "Epoch 220, Training Loss: 3.0501\n",
      "tensor([0.8885, 1.3138, 1.1347, 0.4187, 1.2640, 1.3893, 1.2619, 1.7043, 1.0957,\n",
      "        0.5964], grad_fn=<DiagBackward0>)\n",
      "Epoch 240, Training Loss: 3.0501\n",
      "tensor([0.8885, 1.3138, 1.1347, 0.4187, 1.2640, 1.3893, 1.2619, 1.7043, 1.0957,\n",
      "        0.5964], grad_fn=<DiagBackward0>)\n",
      "Epoch 260, Training Loss: 3.0501\n",
      "tensor([0.8885, 1.3138, 1.1347, 0.4187, 1.2640, 1.3893, 1.2619, 1.7043, 1.0957,\n",
      "        0.5964], grad_fn=<DiagBackward0>)\n",
      "Epoch 280, Training Loss: 3.0501\n",
      "tensor([0.8885, 1.3138, 1.1347, 0.4187, 1.2640, 1.3893, 1.2619, 1.7043, 1.0957,\n",
      "        0.5964], grad_fn=<DiagBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [03:21<05:28, 54.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.1': 5.050203e-07, '0.2': 1.3678467e-06, '0.3': 2.8308523e-05, '0.4': 0.0022805417, '0.5': 5.9100126e-05, '0.6': 8.115813}\n",
      "déviation standard :  0.5\n",
      "matrice à viser :  tensor([1.3649, 1.1231, 1.4335, 0.9464, 0.5563, 1.0852, 0.4625, 1.3749, 1.7731,\n",
      "        2.0437])\n",
      "Epoch 0, Training Loss: 7.5117\n",
      "tensor([1.0500, 1.0500, 1.0500, 0.9500, 0.9500, 1.0500, 0.9500, 1.0500, 1.0500,\n",
      "        1.0500], grad_fn=<DiagBackward0>)\n",
      "Epoch 20, Training Loss: 5.4458\n",
      "tensor([1.3459, 1.1502, 1.4463, 0.9033, 0.3112, 1.0962, 0.1740, 1.3626, 1.8749,\n",
      "        2.0100], grad_fn=<DiagBackward0>)\n",
      "Epoch 40, Training Loss: 5.3951\n",
      "tensor([1.3697, 1.1244, 1.4504, 0.9380, 0.5427, 1.0850, 0.2285, 1.4005, 1.7178,\n",
      "        2.0059], grad_fn=<DiagBackward0>)\n",
      "Epoch 60, Training Loss: 5.3790\n",
      "tensor([1.3680, 1.1206, 1.4363, 0.9521, 0.5638, 1.0845, 0.5066, 1.3819, 1.7816,\n",
      "        2.0685], grad_fn=<DiagBackward0>)\n",
      "Epoch 80, Training Loss: 5.3769\n",
      "tensor([1.3657, 1.1236, 1.4336, 0.9435, 0.5533, 1.0851, 0.4596, 1.3762, 1.7791,\n",
      "        2.0339], grad_fn=<DiagBackward0>)\n",
      "Epoch 100, Training Loss: 5.3764\n",
      "tensor([1.3648, 1.1234, 1.4335, 0.9472, 0.5561, 1.0853, 0.4586, 1.3753, 1.7721,\n",
      "        2.0430], grad_fn=<DiagBackward0>)\n",
      "Epoch 120, Training Loss: 5.3764\n",
      "tensor([1.3647, 1.1229, 1.4337, 0.9462, 0.5571, 1.0853, 0.4648, 1.3752, 1.7722,\n",
      "        2.0450], grad_fn=<DiagBackward0>)\n",
      "Epoch 140, Training Loss: 5.3764\n",
      "tensor([1.3648, 1.1231, 1.4336, 0.9462, 0.5558, 1.0851, 0.4615, 1.3751, 1.7729,\n",
      "        2.0438], grad_fn=<DiagBackward0>)\n",
      "Epoch 160, Training Loss: 5.3764\n",
      "tensor([1.3649, 1.1231, 1.4336, 0.9464, 0.5565, 1.0852, 0.4628, 1.3750, 1.7731,\n",
      "        2.0435], grad_fn=<DiagBackward0>)\n",
      "Epoch 180, Training Loss: 5.3764\n",
      "tensor([1.3649, 1.1231, 1.4335, 0.9464, 0.5563, 1.0852, 0.4624, 1.3749, 1.7731,\n",
      "        2.0436], grad_fn=<DiagBackward0>)\n",
      "Epoch 200, Training Loss: 5.3764\n",
      "tensor([1.3649, 1.1231, 1.4335, 0.9464, 0.5563, 1.0852, 0.4625, 1.3749, 1.7731,\n",
      "        2.0436], grad_fn=<DiagBackward0>)\n",
      "Epoch 220, Training Loss: 5.3764\n",
      "tensor([1.3649, 1.1231, 1.4335, 0.9464, 0.5563, 1.0852, 0.4625, 1.3749, 1.7731,\n",
      "        2.0437], grad_fn=<DiagBackward0>)\n",
      "Epoch 240, Training Loss: 5.3764\n",
      "tensor([1.3649, 1.1231, 1.4335, 0.9464, 0.5563, 1.0852, 0.4625, 1.3749, 1.7731,\n",
      "        2.0437], grad_fn=<DiagBackward0>)\n",
      "Epoch 260, Training Loss: 5.3764\n",
      "tensor([1.3649, 1.1231, 1.4335, 0.9464, 0.5563, 1.0852, 0.4625, 1.3749, 1.7731,\n",
      "        2.0437], grad_fn=<DiagBackward0>)\n",
      "Epoch 280, Training Loss: 5.3764\n",
      "tensor([1.3649, 1.1231, 1.4335, 0.9464, 0.5563, 1.0852, 0.4625, 1.3749, 1.7731,\n",
      "        2.0437], grad_fn=<DiagBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [04:28<04:56, 59.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.1': 5.050203e-07, '0.2': 1.3678467e-06, '0.3': 2.8308523e-05, '0.4': 0.0022805417, '0.5': 0.006372147, '0.6': 8.115813}\n",
      "déviation standard :  0.6\n",
      "matrice à viser :  tensor([ 0.6818, -0.0858,  0.4023,  1.1627,  1.1247,  1.2962,  0.6249,  0.7187,\n",
      "         1.1838,  1.9999])\n",
      "Epoch 0, Training Loss: 4.8484\n",
      "tensor([0.9500, 0.9500, 0.9500, 1.0500, 1.0500, 1.0500, 0.9500, 0.9500, 1.0500,\n",
      "        1.0500], grad_fn=<DiagBackward0>)\n",
      "Epoch 20, Training Loss: 3.3721\n",
      "tensor([0.5462, 0.1722, 0.2633, 1.1735, 1.1520, 1.2510, 0.3798, 0.6658, 1.1686,\n",
      "        2.0078], grad_fn=<DiagBackward0>)\n",
      "Epoch 40, Training Loss: 3.3473\n",
      "tensor([ 0.7138, -0.0917,  0.3090,  1.1730,  1.1229,  1.2982,  0.6816,  0.7095,\n",
      "         1.1839,  1.9564], grad_fn=<DiagBackward0>)\n",
      "Epoch 60, Training Loss: 3.3436\n",
      "tensor([ 0.6815, -0.1291,  0.4318,  1.1627,  1.1229,  1.2996,  0.5984,  0.7297,\n",
      "         1.1882,  2.0247], grad_fn=<DiagBackward0>)\n",
      "Epoch 80, Training Loss: 3.3421\n",
      "tensor([ 0.6761, -0.1120,  0.3997,  1.1618,  1.1253,  1.2969,  0.6352,  0.7194,\n",
      "         1.1837,  1.9901], grad_fn=<DiagBackward0>)\n",
      "Epoch 100, Training Loss: 3.3417\n",
      "tensor([ 0.6827, -0.0943,  0.4001,  1.1630,  1.1248,  1.2958,  0.6223,  0.7175,\n",
      "         1.1834,  1.9999], grad_fn=<DiagBackward0>)\n",
      "Epoch 120, Training Loss: 3.3417\n",
      "tensor([ 0.6825, -0.0864,  0.4036,  1.1626,  1.1245,  1.2958,  0.6247,  0.7182,\n",
      "         1.1839,  2.0012], grad_fn=<DiagBackward0>)\n",
      "Epoch 140, Training Loss: 3.3417\n",
      "tensor([ 0.6818, -0.0850,  0.4017,  1.1626,  1.1247,  1.2961,  0.6254,  0.7185,\n",
      "         1.1837,  1.9999], grad_fn=<DiagBackward0>)\n",
      "Epoch 160, Training Loss: 3.3417\n",
      "tensor([ 0.6817, -0.0855,  0.4025,  1.1627,  1.1247,  1.2962,  0.6248,  0.7186,\n",
      "         1.1838,  1.9997], grad_fn=<DiagBackward0>)\n",
      "Epoch 180, Training Loss: 3.3417\n",
      "tensor([ 0.6817, -0.0858,  0.4022,  1.1627,  1.1247,  1.2962,  0.6249,  0.7187,\n",
      "         1.1838,  1.9999], grad_fn=<DiagBackward0>)\n",
      "Epoch 200, Training Loss: 3.3417\n",
      "tensor([ 0.6817, -0.0859,  0.4023,  1.1626,  1.1247,  1.2961,  0.6249,  0.7187,\n",
      "         1.1838,  1.9999], grad_fn=<DiagBackward0>)\n",
      "Epoch 220, Training Loss: 3.3417\n",
      "tensor([ 0.6817, -0.0859,  0.4022,  1.1627,  1.1247,  1.2962,  0.6249,  0.7187,\n",
      "         1.1838,  1.9999], grad_fn=<DiagBackward0>)\n",
      "Epoch 240, Training Loss: 3.3418\n",
      "tensor([ 0.6818, -0.0858,  0.4023,  1.1627,  1.1247,  1.2961,  0.6249,  0.7187,\n",
      "         1.1837,  1.9999], grad_fn=<DiagBackward0>)\n",
      "Epoch 260, Training Loss: 3.3417\n",
      "tensor([ 0.6818, -0.0858,  0.4022,  1.1627,  1.1247,  1.2962,  0.6249,  0.7187,\n",
      "         1.1837,  1.9999], grad_fn=<DiagBackward0>)\n",
      "Epoch 280, Training Loss: 3.3417\n",
      "tensor([ 0.6818, -0.0858,  0.4023,  1.1627,  1.1247,  1.2961,  0.6249,  0.7187,\n",
      "         1.1838,  1.9999], grad_fn=<DiagBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [05:21<03:48, 57.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.1': 5.050203e-07, '0.2': 1.3678467e-06, '0.3': 2.8308523e-05, '0.4': 0.0022805417, '0.5': 0.006372147, '0.6': 0.00024380008}\n",
      "déviation standard :  0.7\n",
      "matrice à viser :  tensor([1.4091, 1.4113, 0.2697, 1.2885, 1.9207, 0.6526, 1.6692, 2.2298, 0.4966,\n",
      "        2.3726])\n",
      "Epoch 0, Training Loss: 19.2919\n",
      "tensor([1.0500, 1.0500, 0.9500, 1.0500, 1.0500, 0.9500, 1.0500, 1.0500, 0.9500,\n",
      "        1.0500], grad_fn=<DiagBackward0>)\n",
      "Epoch 20, Training Loss: 13.8469\n",
      "tensor([1.0101, 1.3923, 0.0509, 0.8013, 1.6461, 0.2649, 1.6253, 1.9330, 0.1186,\n",
      "        1.9459], grad_fn=<DiagBackward0>)\n",
      "Epoch 40, Training Loss: 12.6672\n",
      "tensor([ 0.9527,  1.4150, -0.6533,  0.6256,  1.9013,  0.4436,  1.6347,  2.2668,\n",
      "        -0.7152,  2.4462], grad_fn=<DiagBackward0>)\n",
      "Epoch 60, Training Loss: 12.4426\n",
      "tensor([ 1.3519,  1.4128, -1.0793,  1.0332,  1.9128,  0.2524,  1.6659,  2.2363,\n",
      "        -1.0672,  2.3676], grad_fn=<DiagBackward0>)\n",
      "Epoch 80, Training Loss: 12.3819\n",
      "tensor([ 1.3824,  1.4119, -1.2551,  1.3012,  1.9183,  0.4133,  1.6708,  2.2233,\n",
      "        -1.1587,  2.3635], grad_fn=<DiagBackward0>)\n",
      "Epoch 100, Training Loss: 12.3698\n",
      "tensor([ 1.4144,  1.4104, -1.3285,  1.2860,  1.9228,  0.5660,  1.6687,  2.2272,\n",
      "        -1.0875,  2.3702], grad_fn=<DiagBackward0>)\n",
      "Epoch 120, Training Loss: 12.3631\n",
      "tensor([ 1.4136,  1.4107, -1.3628,  1.2885,  1.9209,  0.6444,  1.6683,  2.2293,\n",
      "        -0.9586,  2.3729], grad_fn=<DiagBackward0>)\n",
      "Epoch 140, Training Loss: 12.3589\n",
      "tensor([ 1.4095,  1.4110, -1.3707,  1.2884,  1.9207,  0.6529,  1.6680,  2.2298,\n",
      "        -0.7917,  2.3728], grad_fn=<DiagBackward0>)\n",
      "Epoch 160, Training Loss: 12.3545\n",
      "tensor([ 1.4086,  1.4110, -1.3569,  1.2882,  1.9205,  0.6472,  1.6680,  2.2298,\n",
      "        -0.5918,  2.3727], grad_fn=<DiagBackward0>)\n",
      "Epoch 180, Training Loss: 12.3483\n",
      "tensor([ 1.4084,  1.4111, -1.3161,  1.2884,  1.9205,  0.6471,  1.6681,  2.2298,\n",
      "        -0.3562,  2.3726], grad_fn=<DiagBackward0>)\n",
      "Epoch 200, Training Loss: 12.3381\n",
      "tensor([ 1.4085,  1.4111, -1.2262,  1.2884,  1.9206,  0.6495,  1.6683,  2.2298,\n",
      "        -0.0760,  2.3726], grad_fn=<DiagBackward0>)\n",
      "Epoch 220, Training Loss: 12.3207\n",
      "tensor([ 1.4087,  1.4112, -1.0387,  1.2885,  1.9206,  0.6514,  1.6686,  2.2298,\n",
      "         0.2452,  2.3726], grad_fn=<DiagBackward0>)\n",
      "Epoch 240, Training Loss: 12.2961\n",
      "tensor([ 1.4090,  1.4113, -0.6607,  1.2884,  1.9207,  0.6524,  1.6689,  2.2298,\n",
      "         0.5070,  2.3726], grad_fn=<DiagBackward0>)\n",
      "Epoch 260, Training Loss: 12.2730\n",
      "tensor([ 1.4091,  1.4113, -0.0800,  1.2884,  1.9207,  0.6526,  1.6691,  2.2298,\n",
      "         0.4923,  2.3726], grad_fn=<DiagBackward0>)\n",
      "Epoch 280, Training Loss: 12.2650\n",
      "tensor([1.4091, 1.4113, 0.3365, 1.2885, 1.9207, 0.6526, 1.6692, 2.2298, 0.4962,\n",
      "        2.3726], grad_fn=<DiagBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [06:39<03:12, 64.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.1': 5.050203e-07, '0.2': 1.3678467e-06, '0.3': 2.8308523e-05, '0.4': 0.0022805417, '0.5': 0.006372147, '0.6': 0.00024380008, '0.7': 0.011510117}\n",
      "déviation standard :  0.8\n",
      "matrice à viser :  tensor([ 1.1060, -0.3010,  0.3696, -0.2230,  0.2379,  0.7302, -0.1025, -0.1084,\n",
      "         0.3811,  2.1456])\n",
      "Epoch 0, Training Loss: 5.9109\n",
      "tensor([1.0500, 0.9500, 0.9500, 0.9500, 0.9500, 0.9500, 0.9500, 0.9500, 0.9500,\n",
      "        1.0500], grad_fn=<DiagBackward0>)\n",
      "Epoch 20, Training Loss: 3.3209\n",
      "tensor([1.1373, 0.1534, 0.1802, 0.1489, 0.2008, 0.6061, 0.1238, 0.1700, 0.2731,\n",
      "        2.0279], grad_fn=<DiagBackward0>)\n",
      "Epoch 40, Training Loss: 3.1929\n",
      "tensor([ 1.1240, -0.1645,  0.1523, -0.1533,  0.0825,  0.7436, -0.1744, -0.1015,\n",
      "         0.2811,  2.1404], grad_fn=<DiagBackward0>)\n",
      "Epoch 60, Training Loss: 3.1826\n",
      "tensor([ 1.1150, -0.2593,  0.3702, -0.2258,  0.1959,  0.7435, -0.1967, -0.1463,\n",
      "         0.4040,  2.1597], grad_fn=<DiagBackward0>)\n",
      "Epoch 80, Training Loss: 3.1820\n",
      "tensor([ 1.1047, -0.2873,  0.3901, -0.2351,  0.2535,  0.7297, -0.1595, -0.1342,\n",
      "         0.3830,  2.1337], grad_fn=<DiagBackward0>)\n",
      "Epoch 100, Training Loss: 3.1813\n",
      "tensor([ 1.1053, -0.2962,  0.3611, -0.2313,  0.2413,  0.7286, -0.1254, -0.1183,\n",
      "         0.3775,  2.1483], grad_fn=<DiagBackward0>)\n",
      "Epoch 120, Training Loss: 3.1812\n",
      "tensor([ 1.1063, -0.2994,  0.3711, -0.2269,  0.2353,  0.7295, -0.1075, -0.1101,\n",
      "         0.3827,  2.1465], grad_fn=<DiagBackward0>)\n",
      "Epoch 140, Training Loss: 3.1812\n",
      "tensor([ 1.1058, -0.3005,  0.3698, -0.2244,  0.2381,  0.7299, -0.1019, -0.1079,\n",
      "         0.3806,  2.1451], grad_fn=<DiagBackward0>)\n",
      "Epoch 160, Training Loss: 3.1812\n",
      "tensor([ 1.1060, -0.3009,  0.3694, -0.2233,  0.2381,  0.7301, -0.1016, -0.1080,\n",
      "         0.3813,  2.1454], grad_fn=<DiagBackward0>)\n",
      "Epoch 180, Training Loss: 3.1812\n",
      "tensor([ 1.1060, -0.3010,  0.3697, -0.2230,  0.2377,  0.7301, -0.1022, -0.1083,\n",
      "         0.3811,  2.1456], grad_fn=<DiagBackward0>)\n",
      "Epoch 200, Training Loss: 3.1812\n",
      "tensor([ 1.1060, -0.3010,  0.3696, -0.2230,  0.2379,  0.7301, -0.1025, -0.1085,\n",
      "         0.3811,  2.1456], grad_fn=<DiagBackward0>)\n",
      "Epoch 220, Training Loss: 3.1812\n",
      "tensor([ 1.1060, -0.3010,  0.3696, -0.2230,  0.2379,  0.7301, -0.1025, -0.1085,\n",
      "         0.3811,  2.1456], grad_fn=<DiagBackward0>)\n",
      "Epoch 240, Training Loss: 3.1812\n",
      "tensor([ 1.1060, -0.3010,  0.3696, -0.2230,  0.2379,  0.7302, -0.1025, -0.1084,\n",
      "         0.3811,  2.1456], grad_fn=<DiagBackward0>)\n",
      "Epoch 260, Training Loss: 3.1812\n",
      "tensor([ 1.1060, -0.3010,  0.3696, -0.2230,  0.2379,  0.7302, -0.1025, -0.1084,\n",
      "         0.3811,  2.1456], grad_fn=<DiagBackward0>)\n",
      "Epoch 280, Training Loss: 3.1812\n",
      "tensor([ 1.1060, -0.3010,  0.3696, -0.2230,  0.2379,  0.7302, -0.1025, -0.1084,\n",
      "         0.3811,  2.1456], grad_fn=<DiagBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [07:38<02:04, 62.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.1': 5.050203e-07, '0.2': 1.3678467e-06, '0.3': 2.8308523e-05, '0.4': 0.0022805417, '0.5': 0.006372147, '0.6': 0.00024380008, '0.7': 0.011510117, '0.8': 0.0006642968}\n",
      "déviation standard :  0.9\n",
      "matrice à viser :  tensor([-1.8208, -0.7000,  0.9083,  1.5268,  2.2383,  1.3132, -0.0305,  0.6117,\n",
      "         1.2906,  1.7606])\n",
      "Epoch 0, Training Loss: 10.6532\n",
      "tensor([0.9500, 0.9500, 0.9500, 1.0500, 1.0500, 1.0500, 0.9500, 0.9500, 1.0500,\n",
      "        1.0500], grad_fn=<DiagBackward0>)\n",
      "Epoch 20, Training Loss: 7.2935\n",
      "tensor([0.0935, 0.1005, 0.3025, 1.4187, 1.9893, 1.2653, 0.1753, 0.2935, 1.1631,\n",
      "        1.7077], grad_fn=<DiagBackward0>)\n",
      "Epoch 40, Training Loss: 6.8366\n",
      "tensor([-0.4091, -0.3761,  0.2692,  1.5584,  2.2649,  1.3031, -0.1450,  0.0386,\n",
      "         1.2929,  1.7529], grad_fn=<DiagBackward0>)\n",
      "Epoch 60, Training Loss: 6.7033\n",
      "tensor([-0.6400, -0.5515,  1.0132,  1.5345,  2.2507,  1.3106, -0.2600,  0.5565,\n",
      "         1.2981,  1.7684], grad_fn=<DiagBackward0>)\n",
      "Epoch 80, Training Loss: 6.6893\n",
      "tensor([-0.7609, -0.6096,  0.8693,  1.5250,  2.2272,  1.3118, -0.1889,  0.5993,\n",
      "         1.2937,  1.7612], grad_fn=<DiagBackward0>)\n",
      "Epoch 100, Training Loss: 6.6851\n",
      "tensor([-0.8460, -0.6332,  0.9011,  1.5257,  2.2374,  1.3124, -0.0922,  0.6217,\n",
      "         1.2913,  1.7600], grad_fn=<DiagBackward0>)\n",
      "Epoch 120, Training Loss: 6.6841\n",
      "tensor([-0.9179, -0.6479,  0.9135,  1.5265,  2.2395,  1.3131, -0.0376,  0.6051,\n",
      "         1.2904,  1.7602], grad_fn=<DiagBackward0>)\n",
      "Epoch 140, Training Loss: 6.6835\n",
      "tensor([-0.9819, -0.6593,  0.9096,  1.5267,  2.2387,  1.3132, -0.0251,  0.6140,\n",
      "         1.2904,  1.7604], grad_fn=<DiagBackward0>)\n",
      "Epoch 160, Training Loss: 6.6831\n",
      "tensor([-1.0398, -0.6686,  0.9080,  1.5268,  2.2383,  1.3132, -0.0282,  0.6118,\n",
      "         1.2905,  1.7605], grad_fn=<DiagBackward0>)\n",
      "Epoch 180, Training Loss: 6.6828\n",
      "tensor([-1.0924, -0.6761,  0.9081,  1.5268,  2.2382,  1.3131, -0.0307,  0.6114,\n",
      "         1.2906,  1.7606], grad_fn=<DiagBackward0>)\n",
      "Epoch 200, Training Loss: 6.6826\n",
      "tensor([-1.1404, -0.6822,  0.9082,  1.5268,  2.2382,  1.3132, -0.0308,  0.6118,\n",
      "         1.2906,  1.7606], grad_fn=<DiagBackward0>)\n",
      "Epoch 220, Training Loss: 6.6824\n",
      "tensor([-1.1843, -0.6869,  0.9083,  1.5268,  2.2383,  1.3131, -0.0305,  0.6118,\n",
      "         1.2906,  1.7606], grad_fn=<DiagBackward0>)\n",
      "Epoch 240, Training Loss: 6.6822\n",
      "tensor([-1.2246, -0.6905,  0.9083,  1.5268,  2.2383,  1.3132, -0.0305,  0.6117,\n",
      "         1.2906,  1.7606], grad_fn=<DiagBackward0>)\n",
      "Epoch 260, Training Loss: 6.6821\n",
      "tensor([-1.2618, -0.6933,  0.9083,  1.5268,  2.2383,  1.3132, -0.0305,  0.6117,\n",
      "         1.2906,  1.7606], grad_fn=<DiagBackward0>)\n",
      "Epoch 280, Training Loss: 6.6820\n",
      "tensor([-1.2961, -0.6953,  0.9083,  1.5268,  2.2383,  1.3132, -0.0305,  0.6117,\n",
      "         1.2906,  1.7606], grad_fn=<DiagBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [08:37<01:01, 61.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.1': 5.050203e-07, '0.2': 1.3678467e-06, '0.3': 2.8308523e-05, '0.4': 0.0022805417, '0.5': 0.006372147, '0.6': 0.00024380008, '0.7': 0.011510117, '0.8': 0.0006642968, '0.9': 0.4945909}\n",
      "déviation standard :  1.0\n",
      "matrice à viser :  tensor([ 1.7398,  0.1201,  0.5709, -0.3887,  0.9151, -0.4653,  3.6877,  1.4519,\n",
      "         1.7332,  1.7098])\n",
      "Epoch 0, Training Loss: 143.6058\n",
      "tensor([1.0500, 0.9500, 0.9500, 0.9500, 0.9500, 0.9500, 1.0500, 1.0500, 1.0500,\n",
      "        1.0500], grad_fn=<DiagBackward0>)\n",
      "Epoch 20, Training Loss: 101.1745\n",
      "tensor([1.4905, 0.1450, 0.3442, 0.1258, 0.5970, 0.1062, 2.0840, 1.2063, 1.5145,\n",
      "        1.4912], grad_fn=<DiagBackward0>)\n",
      "Epoch 40, Training Loss: 83.6345\n",
      "tensor([ 0.6171, -0.8282, -0.6908, -0.6960, -0.5088, -0.8955,  2.8434,  0.1970,\n",
      "         0.6078,  0.5871], grad_fn=<DiagBackward0>)\n",
      "Epoch 60, Training Loss: 79.1542\n",
      "tensor([ 0.3232, -1.5315, -1.2981, -1.4849, -1.2717, -1.5534,  3.2984, -0.2430,\n",
      "         0.3439,  0.2946], grad_fn=<DiagBackward0>)\n",
      "Epoch 80, Training Loss: 77.4477\n",
      "tensor([ 0.2033, -1.9347, -1.6279, -1.9488, -1.6531, -1.9292,  3.5349, -0.4208,\n",
      "         0.2225,  0.1645], grad_fn=<DiagBackward0>)\n",
      "Epoch 100, Training Loss: 76.7984\n",
      "tensor([ 0.1969, -2.1456, -1.8018, -2.1841, -1.8562, -2.1370,  3.6383, -0.4932,\n",
      "         0.2220,  0.1492], grad_fn=<DiagBackward0>)\n",
      "Epoch 120, Training Loss: 76.5906\n",
      "tensor([ 0.2275, -2.2656, -1.8943, -2.3244, -1.9597, -2.2562,  3.6737, -0.5035,\n",
      "         0.2612,  0.1745], grad_fn=<DiagBackward0>)\n",
      "Epoch 140, Training Loss: 76.5290\n",
      "tensor([ 0.2828, -2.3523, -1.9608, -2.4264, -2.0287, -2.3354,  3.6838, -0.4912,\n",
      "         0.3234,  0.2244], grad_fn=<DiagBackward0>)\n",
      "Epoch 160, Training Loss: 76.5013\n",
      "tensor([ 0.3464, -2.4305, -2.0200, -2.5149, -2.0867, -2.3970,  3.6865, -0.4688,\n",
      "         0.3962,  0.2835], grad_fn=<DiagBackward0>)\n",
      "Epoch 180, Training Loss: 76.4779\n",
      "tensor([ 0.4158, -2.5094, -2.0790, -2.6009, -2.1411, -2.4511,  3.6873, -0.4413,\n",
      "         0.4750,  0.3486], grad_fn=<DiagBackward0>)\n",
      "Epoch 200, Training Loss: 76.4536\n",
      "tensor([ 0.4897, -2.5912, -2.1395, -2.6890, -2.1931, -2.5022,  3.6874, -0.4102,\n",
      "         0.5586,  0.4180], grad_fn=<DiagBackward0>)\n",
      "Epoch 220, Training Loss: 76.4282\n",
      "tensor([ 0.5675, -2.6762, -2.2017, -2.7804, -2.2421, -2.5525,  3.6875, -0.3756,\n",
      "         0.6462,  0.4912], grad_fn=<DiagBackward0>)\n",
      "Epoch 240, Training Loss: 76.4015\n",
      "tensor([ 0.6489, -2.7639, -2.2654, -2.8750, -2.2869, -2.6033,  3.6874, -0.3373,\n",
      "         0.7371,  0.5677], grad_fn=<DiagBackward0>)\n",
      "Epoch 260, Training Loss: 76.3738\n",
      "tensor([ 0.7334, -2.8542, -2.3301, -2.9723, -2.3264, -2.6549,  3.6874, -0.2953,\n",
      "         0.8307,  0.6472], grad_fn=<DiagBackward0>)\n",
      "Epoch 280, Training Loss: 76.3451\n",
      "tensor([ 0.8205, -2.9464, -2.3954, -3.0720, -2.3595, -2.7075,  3.6875, -0.2494,\n",
      "         0.9260,  0.7292], grad_fn=<DiagBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [12:00<00:00, 72.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.1': 5.050203e-07, '0.2': 1.3678467e-06, '0.3': 2.8308523e-05, '0.4': 0.0022805417, '0.5': 0.006372147, '0.6': 0.00024380008, '0.7': 0.011510117, '0.8': 0.0006642968, '0.9': 0.4945909, '1.0': 19.082157}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples=100\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for stddev in tqdm(stddevs):\n",
    "    print(\"déviation standard : \",str(stddev))\n",
    "    \n",
    "    diag = torch.from_numpy(np.random.normal(loc=mean, scale=stddev, size=dim).astype(np.float32))\n",
    "\n",
    "    A_true = torch.diag(diag)\n",
    "    print(\"matrice à viser : \" , diag)\n",
    "\n",
    "\n",
    "    y_data = phi_A(x_data,A_true)\n",
    "    \n",
    "    \n",
    "    neural_ode = NeuralODE(np.eye(dim).astype(np.float32))\n",
    "    training_loss=train_model(neural_ode, x_data,y_data)\n",
    "    A_estimated = neural_ode.get_A()\n",
    "\n",
    "    #ajout des résultats pour chaque n_samples\n",
    "    training_losses[str(stddev)]=training_loss\n",
    "    frob_losses[str(stddev)]=np.linalg.norm((A_true-A_estimated).detach().numpy())\n",
    "    print(frob_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0c1814d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frob_losses_500=frob_losses\n",
    "keys = [float(k) for k in frob_losses.keys()]\n",
    "values = list(frob_losses.values())\n",
    "\n",
    "plt.plot(keys, values)\n",
    "plt.title(\"evolution of the error with the standard deviation\")\n",
    "plt.xlabel('Standard deviation')\n",
    "plt.ylabel('Frobenius norm of A_true-A_n')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
